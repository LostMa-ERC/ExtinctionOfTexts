import random
import torch
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
import birth_death_utils as bd
import os
from collections import Counter
from tqdm import tqdm
import pickle
from itertools import groupby
import pandas as pd
import yaml
import re
import multiprocessing as mp
import seaborn as sns

from sbi.analysis import pairplot
from sbi.inference import NPE, simulate_for_sbi, NLE
from sbi.utils import BoxUniform

from sbi.utils.user_input_checks import (
    check_sbi_inputs,
    process_prior,
    process_simulator,
)

from torch import Tensor

import multiprocessing

def compute_summary_stats(g):
    """
    Generate a vector of summary statistics from a graph generated by birth-death simulation

    Parameters
    ----------
    g : nx.DiGraph()
        tree graph generated by birth_death_utils.ct_bd_tree

    Returns
    -------
    list
        vector of summary statistics characterizing a single tradition
    """
    n_living = list(nx.get_node_attributes(g, 'state').values()).count(True)

    if n_living == 0:
        return None

    if n_living == 1:
        return [1,0, -1,-1,-1,-1,-1,-1,-1]

    if n_living == 2:
        birth_times_trad = []
        for n in g.nodes():
            if g.nodes[n]['state']:
                birth_times_trad.append(g.nodes[n]['birth_time'])
        timelapse = int(max(birth_times_trad)-min(birth_times_trad))
        return [2, timelapse, -1,-1,-1,-1,-1,-1,-1]

    if n_living >= 3:
        birth_times_trad = []
        degrees = []
        direct_filiation_nb = 0
        arch_dists = []
        st = bd.generate_stemma(g)
        archetype = bd.root(st)

        for n in st.nodes():
            degrees.append(st.out_degree(n))

            if n != archetype:
                father = list(st.predecessors(n))[0]
                if st.nodes[n]['state'] and st.nodes[father]['state']:
                    direct_filiation_nb +=1
            if st.nodes[n]['state']:
                birth_times_trad.append(st.nodes[n]['birth_time'])
            arch_dists.append(len(nx.shortest_path(st, source=archetype, target=n)))

        timelapse = int(max(birth_times_trad)-min(birth_times_trad))
        deg_dist = Counter(degrees)
        deg1 = deg_dist[1]
        deg2 = deg_dist[2]
        deg3 = deg_dist[3]
        deg4 = deg_dist[4]
        depth = max(arch_dists)
        n_nodes = len(list(st.nodes()))

        return [
            n_living,
            timelapse,
            n_nodes,
            direct_filiation_nb,
            deg1,
            deg2,
            deg3,
            deg4,
            depth
        ]

lambda_min_prior = 4.*10**(-3)
lambda_max_prior = 9.*10**(-3) 

mu_min_prior = 1.*10**(-3)
mu_max_prior = 5*10**(-3)

decay_min_prior = 0
decay_max_prior = 1

decimation_min_prior = 0
decimation_max_prior = 1

N_samples_prior = 500_000 #500000
N_samples_posterior = 100 #1000

def simulator(theta):
    lda0, mu, decay, decim = theta
    g = bd.generate_tree_unified(lda0, mu, decay, decim, 1000, 1000, 500)

    return g

prior = BoxUniform(low=Tensor([lambda_min_prior, mu_min_prior, decay_min_prior, decimation_min_prior]),
                          high=Tensor([lambda_max_prior, mu_max_prior, decay_max_prior, decimation_max_prior]))

theta0 = prior.sample((N_samples_prior,))

theta  = []
x = []

#for t in tqdm(theta0):
#    vec = compute_summary_stats(simulator(t))
#    if vec != None:
#        theta.append(list(t))
#        x.append(vec)

def process_theta(t):
    """
    Helper function to process a single theta sample.
    """
    G = simulator(t)
    vec = compute_summary_stats(G)
    if vec is not None:
        return (list(t), vec)
    else:
        return None

def parallel_simulate(theta0, n_processes=None):
    """
    Parallelize the simulation and summary statistics computation.

    Args:
        theta0: Tensor of shape (N_samples_prior, 4)
        n_processes: Number of processes to use (default: all available cores)

    Returns:
        theta, x: Lists of valid theta and summary statistics
    """
    if n_processes is None:
        n_processes = multiprocessing.cpu_count()

    with multiprocessing.Pool(n_processes) as pool:
        results = list(tqdm(pool.imap(process_theta, theta0), total=len(theta0)))

    # Filter out None results
    valid_results = [r for r in results if r is not None]
    theta, x = zip(*valid_results) if valid_results else ([], [])

    return list(theta), list(x)

# Example usage:
theta, x = parallel_simulate(theta0, n_processes=7)
theta = torch.tensor(theta, dtype=torch.float32)
x = torch.tensor(x, dtype=torch.float32)

inference = NLE(prior=prior)
inference = inference.append_simulations(Tensor(theta), Tensor(x))
likelihood_estimator = inference.train()
with open("pretrained_models/inference_unif.pickle", "wb") as f:
    pickle.dump(inference, f)

