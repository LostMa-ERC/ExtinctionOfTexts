import random
import torch
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
import birth_death_utils as bd
import os
from collections import Counter
from tqdm.notebook import tqdm
import pickle
from itertools import groupby
import pandas as pd
import yaml
import re
import multiprocessing as mp
import seaborn as sns

from sbi.analysis import pairplot
from sbi.inference import NPE, simulate_for_sbi, NLE
from sbi.utils import BoxUniform

from sbi.utils.user_input_checks import (
    check_sbi_inputs,
    process_prior,
    process_simulator,
)

from torch import Tensor

def compute_summary_stats(g):
    """
    Generate a vector of summary statistics from a graph generated by birth-death simulation

    Parameters
    ----------
    g : nx.DiGraph()
        tree graph generated by birth_death_utils.ct_bd_tree
    
    Returns
    -------
    list
        vector of summary statistics characterizing a single tradition
    """
    n_living = list(nx.get_node_attributes(g, 'state').values()).count(True)

    if n_living == 0:
        return None
    
    if n_living == 1:
        return [1,0, -1,-1,-1,-1,-1,-1,-1]

    if n_living == 2:
        birth_times_trad = []
        for n in g.nodes():
            if g.nodes[n]['state']:
                birth_times_trad.append(g.nodes[n]['birth_time'])
        timelapse = int(max(birth_times_trad)-min(birth_times_trad))
        return [2, timelapse, -1,-1,-1,-1,-1,-1,-1]
    
    if n_living >= 3:
        birth_times_trad = []
        degrees = []
        direct_filiation_nb = 0
        arch_dists = []
        st = bd.generate_stemma(g)
        archetype = bd.root(st)

        for n in st.nodes():
            degrees.append(st.out_degree(n))

            if n != archetype:
                father = list(st.predecessors(n))[0]
                if st.nodes[n]['state'] and st.nodes[father]['state']:
                    direct_filiation_nb +=1
            if st.nodes[n]['state']:
                birth_times_trad.append(st.nodes[n]['birth_time'])
            arch_dists.append(len(nx.shortest_path(st, source=archetype, target=n)))
        
        timelapse = int(max(birth_times_trad)-min(birth_times_trad))
        deg_dist = Counter(degrees)
        deg1 = deg_dist[1]
        deg2 = deg_dist[2]
        deg3 = deg_dist[3]
        deg4 = deg_dist[4]
        depth = max(arch_dists)
        n_nodes = len(list(st.nodes()))

        return [
            n_living,
            timelapse,
            n_nodes,
            direct_filiation_nb,
            deg1,
            deg2,
            deg3,
            deg4,
            depth
        ]

def convert_date(x):
    pattern1 = re.compile('[0-9][0-9][0-9][0-9]')
    pattern2 = re.compile('[0-9][0-9][0-9][0-9]-[0-9][0-9][0-9][0-9]')

    if bool(pattern1.fullmatch(x)):
        return int(x)
    if bool(pattern2.fullmatch(x)):
        xx = x.split('-')
        return (int(xx[0]),int(xx[1]))

def top(x):
    '''
    Upper bound on witness date (single year or range)
    '''
    match x:
        case (x1, x2):
            return x2
        case _:
            return x

def bottom(x):
    '''
    Lower bound on witness date (single year or range)
    '''
    match x:
        case (x1, x2):
            return x1
        case _:
            return x


def expected_abs_diff_degenerate(c, a, b):
    '''
    Returns the expectation value of the timelapse between one date given as range
    and one other as a single year
    '''
    if a == b:
        return abs(a - c)
    if a <= c <= b:
        return ((c-a)**2 + (b-c)**2) / (2*(b-a))
    elif c < a:
        return (a+b)/2 - c
    else: 
        return c - (a+b)/2

def expected_abs_diff(a, b, c, d):
    '''
    Return the expectation value of the timelapse between two dates given as
    intervals
    '''
    if a == b:
        return expected_abs_diff_degenerate(a, c, d)
    if c == d:
        return expected_abs_diff_degenerate(c, a, b)
    elif a < b < c < d:
        return (c+d-b-a)/2
    elif a <= c <= d < b:
        return (1/(b-a)) * ((1/2)*((d-a)*(c-a)+(b-d)*(b-c)) + (1/3)*(d-c)**2)
    elif a <= c <= b <= d:
        return (1/(b-a)) * ((1/2)* ((c-a)*(d-a) + (b-c)*(d-b)) + (1/(3*(d-c)))*(b-c)**3 )
    else:
        print(f"{a},{b} -- {c},{d} ")
        raise ValueError("Must have a < b, c < d, and a < c")

wholeCorpus = {}
corpus_dates = {}
for work in os.listdir(f'corpus_stemmata/'):
    print(f'{work}')
    st = bd.load_from_OpenStemmata(f'corpus_stemmata/{work}/stemma.gv')
    with open(f"corpus_stemmata/{work}/metadata.txt", 'r') as f:
        content = f.read()
    metadata = yaml.safe_load(content)
    if "wits" in metadata:
        dates = [wit["witOrigDate"] for wit in metadata['wits'] if wit["witOrigDate"] != '']
    else:
        dates = []

    dates_num = []
    for x in dates:
        if x != '':
            date_num = convert_date(x)
            if date_num != None:
                dates_num.append(date_num)
 
    wholeCorpus[f"{work}"] = st
    corpus_dates[f"{work}"] = dates_num

ranges_per_work = {}
for work, t_dates in corpus_dates.items():
    if t_dates != []:
        lb = sorted(t_dates, key=bottom)[0]
        ub = sorted(t_dates, key=top)[-1]
        ranges_per_work[work] = (lb,ub)


lifespans = {}
for w,v in ranges_per_work.items():
    match v:
        case [(a,b), (c,d)]:
            lifespans[w] = expected_abs_diff(a,b,c,d)
        case [(a,b), c]:
            lifespans[w] = expected_abs_diff_degenerate(a,b,c)
        case [c,(a,b)]:
            lifespans[w] = expected_abs_diff_degenerate(a,b,c)
        case [a,b]:
            lifespans[w] = abs(a-b)
        case _:
            print('error')

x_obs0 = {}
sizes = {}
for k in lifespans.keys():
    ## computation of observables
    g = wholeCorpus[k]

    n_living = list(nx.get_node_attributes(g, 'state').values()).count(True)
    sizes[k] = n_living
    degrees = []
    direct_filiation_nb = 0
    arch_dists = []

    if n_living >= 3:
        st = bd.generate_stemma(g)
        archetype = bd.root(st)
        for n in st.nodes():
            degrees.append(st.out_degree(n))

            if n != archetype:
                father = list(st.predecessors(n))[0]
                if st.nodes[n]['state'] and st.nodes[father]['state']:
                    direct_filiation_nb +=1
            arch_dists.append(len(nx.shortest_path(st, source=archetype, target=n)))
        
        timelapse = lifespans[k]
        deg_dist = Counter(degrees)
        deg1 = deg_dist[1]
        deg2 = deg_dist[2]
        deg3 = deg_dist[3]
        deg4 = deg_dist[4]
        depth = max(arch_dists)
        n_nodes = len(list(st.nodes()))

        x_obs0[k] = [
            n_living,
            4*int(timelapse),
            n_nodes,
            direct_filiation_nb,
            deg1,
            deg2,
            deg3,
            deg4,
            depth
        ]
df = pd.read_csv("Old_French_witnesses.csv")
f2_works = []
f1_works = []
works = set(list(df['text H-ID']))
size_frags_d = []
size_d = []
for work in works:
    n_wit = len(df[(df['text H-ID'] == work) & (df['status'] != 'fragment')])
    n_frags = len(df[(df['text H-ID'] == work) & (df['status'] == 'fragment')])
    if n_wit !=0:
        size_d.append(n_wit)
    if n_frags !=0:
        size_frags_d.append(n_frags)

    if n_wit == 2:
        f2_works.append(work)
    if n_wit == 1:
        f1_works.append(work)

size_dist = Counter(size_d)
size_dist_frags = Counter(size_frags_d)

f2_dates_0 = [df[(df["text H-ID"] == x) & (df['status'] != 'fragment')]["Date"].values.tolist() for x in f2_works]
f2_dates = [list(map(convert_date, x)) for  x in f2_dates_0]

ranges_per_work = []
for t_dates in f2_dates:
    if t_dates != []:
        lb = sorted(t_dates, key=bottom)[0]
        ub = sorted(t_dates, key=top)[-1]
        ranges_per_work.append((lb,ub))


lifespans_f2 = []
for v in ranges_per_work:
    match v:
        case [(a,b), (c,d)]:
            lifespans_f2.append(expected_abs_diff(a,b,c,d))
        case [(a,b), c]:
            lifespans_f2.append(expected_abs_diff_degenerate(a,b,c))
        case [c,(a,b)]:
            lifespans_f2.append(expected_abs_diff_degenerate(a,b,c))
        case [a,b]:
            lifespans_f2.append(abs(a-b))
        case _:
            print('error')

add_f1 = [[
        1,
        0,
        -1,
        -1,
        -1,
        -1,
        -1,
        -1,
        -1] for k in range(61)]

add_f2 = [[2, int(4 * n), -1,-1,-1,-1,-1,-1,-1] for n in lifespans_f2]
x_obs_empirical = list(x_obs0.values()) + add_f1 + add_f2[:17]

random.shuffle(x_obs_empirical)

lambda_min_prior = 4.*10**(-3)
lambda_max_prior = 12.*10**(-3)

mu_min_prior = 1.*10**(-3)
mu_max_prior = 5*10**(-3)

decay_min_prior = 0
decay_max_prior = 1

decimation_min_prior = 0
decimation_max_prior = 1

N_samples_prior = 500_000 #500000
N_samples_posterior = 1000 #1000

#load pretrained model serialized as pickle object
with open("pretrained_models/inference_unif.pickle", "rb") as f:
    inference = pickle.load(f)


#posterior = inference.build_posterior(sample_with="rejection")
posterior = inference.build_posterior()

samples = posterior.sample(
    (N_samples_posterior,),
    x=x_obs_empirical
)

# Generate the pair plot
fig, axes = pairplot(
    samples,
    limits=[
        [lambda_min_prior, lambda_max_prior],
        [mu_min_prior, mu_max_prior],
        [decay_min_prior, decay_max_prior],
        [decimation_min_prior, decimation_max_prior]
    ],
    figsize=(10, 10),  # Larger figsize for better readability
    labels=[r"$\lambda$", r"$\mu$", r"$r_{\text{decay}}$", r"$r_{\text{decim}}$"]
)

# Adjust the DPI and save the figure
plt.savefig(
    "pairplot_highres.pdf",  # Save as PDF for vector graphics (best for papers)
    dpi=300,  # High DPI for high resolution
    bbox_inches="tight",  # Remove extra whitespace
    format="pdf"  # Use PDF for publication-quality figures
)

# Optionally, save as PNG (if needed)
plt.savefig(
    "pairplot_highres.png",
    dpi=300,
    bbox_inches="tight"
)

